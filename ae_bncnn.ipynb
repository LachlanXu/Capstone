{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import sklearn.preprocessing as prep\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fy\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def mapStd(X,X_train,X_valid,X_test):\n",
    "    preprocessor=prep.StandardScaler().fit(X)\n",
    "    X = preprocessor.transform(X)\n",
    "    X_train = preprocessor.transform(X_train)\n",
    "    X_valid = preprocessor.transform(X_valid)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    return X,X_train,X_valid,X_test\n",
    "\n",
    "\n",
    "def mapMinmax(X,X_train,X_valid,X_test):\n",
    "    preprocessor=prep.MinMaxScaler().fit(X)\n",
    "    X = 2*preprocessor.transform(X)-1\n",
    "    X_train = 2*preprocessor.transform(X_train)-1\n",
    "    X_valid = 2*preprocessor.transform(X_valid)-1\n",
    "    X_test = 2*preprocessor.transform(X_test)-1\n",
    "    return X,X_train,X_valid,X_test\n",
    "\n",
    "\n",
    "def load_data(fold):\n",
    "    data = sio.loadmat('./BNC-DGHL/Datasets/ALLASD{}_NETFC_SG_Pear.mat'.format(fold+1))\n",
    "    X = data['net']\n",
    "    X_train = data['net_train']\n",
    "    X_valid = data['net_valid']\n",
    "    X_test = data['net_test']\n",
    "\n",
    "    Idx = [2, 3, 4, 5, 6, 7, 8, 9]  # 3:Age 4:Sex 5:Handedness 6:FIQ 7:VIQ 8:PIQ 9:EYE Status\n",
    "    Y = data['phenotype'][:, Idx]\n",
    "    Y_train = data['phenotype_train'][:, Idx]\n",
    "    Y_valid = data['phenotype_valid'][:, Idx]\n",
    "    Y_test = data['phenotype_test'][:, Idx]\n",
    "    col_idx = [1, 4, 5, 6]  # 3:Age 6:FIQ 7:VIQ 8:PIQ\n",
    "    Y[:, col_idx], Y_train[:, col_idx], \\\n",
    "    Y_valid[:, col_idx], Y_test[:, col_idx] = mapStd(Y[:, col_idx],\n",
    "                                                     Y_train[:, col_idx],\n",
    "                                                     Y_valid[:, col_idx],\n",
    "                                                     Y_test[:, col_idx])\n",
    "    col_idx = [2, 3, 7]\n",
    "    Y[:, col_idx], Y_train[:, col_idx], \\\n",
    "    Y_valid[:, col_idx], Y_test[:, col_idx] = mapMinmax(Y[:, col_idx],\n",
    "                                                        Y_train[:, col_idx],\n",
    "                                                        Y_valid[:, col_idx],\n",
    "                                                        Y_test[:, col_idx])\n",
    "\n",
    "\n",
    "    Y_train2 = data['phenotype_train'][:, 2]\n",
    "    Y_valid2 = data['phenotype_valid'][:, 2]\n",
    "    Y_test2 = data['phenotype_test'][:, 2]\n",
    "\n",
    "    ln = nn.LayerNorm(normalized_shape=[90, 90], elementwise_affine=False)\n",
    "    X_train = ln(torch.tensor(X_train)).view(-1, 1, 90, 90).type(torch.FloatTensor)\n",
    "    X_valid = ln(torch.tensor(X_valid)).view(-1, 1, 90, 90).type(torch.FloatTensor)\n",
    "    X_test = ln(torch.tensor(X_test)).view(-1, 1, 90, 90).type(torch.FloatTensor)\n",
    "    Y_train = torch.tensor(Y_train)\n",
    "    Y_valid = torch.tensor(Y_valid)\n",
    "    Y_test = torch.tensor(Y_test)\n",
    "    Y_train2 = torch.tensor(Y_train2)\n",
    "    Y_valid2 = torch.tensor(Y_valid2)\n",
    "    Y_test2 = torch.tensor(Y_test2)\n",
    "\n",
    "    return X_train, X_valid, X_test, Y_train, Y_valid, Y_test, Y_train2, Y_valid2, Y_test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=0, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.autoencoder = Autoencoder()\n",
    "        for param in self.autoencoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.E2E = nn.Conv2d(1, 32, kernel_size=(90, 1))\n",
    "        # self.E2E = E2EBlock(1, 32, bias=True)\n",
    "        self.E2N = nn.Conv2d(32, 64, kernel_size=(90, 1))\n",
    "        self.N2G = nn.Conv2d(64, 128, kernel_size=(90, 1))\n",
    "        self.fc1 = nn.Linear(128, 96)\n",
    "        self.fc2 = nn.Linear(96, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.autoencoder(x)\n",
    "        x = F.relu(self.E2E(x) + self.E2E(x).transpose(3, 2))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.E2N(x).transpose(3, 2)*2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.N2G(x))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, optimizer, loss_func):\n",
    "    acc_sum = 0.\n",
    "    loss_sum = 0.\n",
    "    net.train()\n",
    "    for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        out = net(batch_x)\n",
    "        loss_step = loss_func(out, batch_y)\n",
    "        loss_step.backward()\n",
    "        optimizer.step()\n",
    "        acc_step = (out.argmax(dim=1) == batch_y).float().mean().item()\n",
    "        loss_sum += loss_step.data.item()\n",
    "        acc_sum += acc_step\n",
    "        # print(\"step {}: loss: {}; acc: {}\".format(epoch + 1, step + 1, loss_step.data.item(), acc_step))\n",
    "    loss = loss_sum / len(train_loader)\n",
    "    acc = acc_sum / len(train_loader)\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def infer(loader, net, loss_func):\n",
    "    acc_sum = 0.\n",
    "    loss_sum = 0.\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            if use_cuda:\n",
    "                batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "            out = net(batch_x)\n",
    "            loss_step = loss_func(out, batch_y)\n",
    "            acc_step = (out.argmax(dim=1) == batch_y).float().mean().item()\n",
    "            loss_sum += loss_step.data.item()\n",
    "            acc_sum += acc_step\n",
    "            # print(\"step {}: loss: {}; acc: {}\".format(epoch + 1, step + 1, loss_step.data.item(), acc_step))\n",
    "        loss = loss_sum / len(loader)\n",
    "        acc = acc_sum / len(loader)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acc = 0\n",
    "for i in range(1):\n",
    "    # 定义参数\n",
    "    fold = 5\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCH = 2000\n",
    "    n_evaluation_epochs = 2\n",
    "    n_patience = 50\n",
    "    ax = [[] for _ in range(fold)]\n",
    "    ay = [[] for _ in range(fold)]\n",
    "    color_list = ['royalblue', 'darkorange', 'mediumpurple', 'forestgreen', 'firebrick']\n",
    "    acc_list = []\n",
    "\n",
    "    # 实例化网络，并且定义loss和优化器\n",
    "    net = Net().to(device)\n",
    "    \n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        net = net.cuda()\n",
    "        #net = torch.nn.DataParallel(net, device_ids=[0])\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=3e-4, momentum=0.9, weight_decay=1e-2)  # 1e-4\n",
    "\n",
    "\n",
    "    for f in range(fold):\n",
    "        net.apply(init_weights)\n",
    "        net.autoencoder.load_state_dict(torch.load(\"C:/Users/xhx20/Desktop/AE_DGHL.pth\"))\n",
    "\n",
    "        X_train, X_valid, X_test, Y_train, Y_valid, Y_test, Y_train2, Y_valid2, Y_test2 = load_data(f)\n",
    "\n",
    "        \"\"\"只用1/5个样本进行学习\"\"\"\n",
    "        fraction = 5\n",
    "        X_train, X_valid, X_test = X_train[:len(X_train)//fraction], X_valid[:len(X_valid)//fraction], X_test[:len(X_test)//fraction]\n",
    "        Y_train2, Y_valid2, Y_test2 = Y_train2[:len(Y_train2)//fraction], Y_valid2[:len(Y_valid2)//fraction], Y_test2[:len(Y_test2)//fraction]\n",
    "\n",
    "\n",
    "        train_dataset = Data.TensorDataset(X_train, Y_train2.long())\n",
    "        train_loader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        valid_dataset = Data.TensorDataset(X_valid, Y_valid2.long())\n",
    "        valid_loader = Data.DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        test_dataset = Data.TensorDataset(X_test, Y_test2.long())\n",
    "        test_loader = Data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        best_valid_acc = 0.0\n",
    "        min_valid_loss = 10000\n",
    "        p = 0\n",
    "        # print(\"-----------------------------------------------------\")\n",
    "        # print(\"fold:\", f+1)\n",
    "        # print(\"-----------------------------------------------------\")\n",
    "\n",
    "        for epoch in range(EPOCH):\n",
    "            train_loss, train_acc = train(train_loader, net, optimizer, loss_func)\n",
    "            # print(\"epoch {} : train_loss= {}; train_acc= {}\".format(epoch + 1, train_loss, train_acc))\n",
    "            valid_loss, valid_acc = infer(valid_loader, net, loss_func)\n",
    "            # print('valid_loss = ', valid_loss, 'valid_acc = ', valid_acc, \"p =\", p)\n",
    "\n",
    "            if epoch % n_evaluation_epochs == 0:\n",
    "                print('Epoch:', epoch, 'p:', p, 'train_loss:', train_loss, 'train_acc:', train_acc, 'valid_loss:', valid_loss, 'valid_acc:', valid_acc)\n",
    "                plt.figure(1)\n",
    "                ax[f].append(epoch + 1)\n",
    "                ay[f].append(np.mean(train_loss))\n",
    "                plt.clf()\n",
    "                for i in range(fold):\n",
    "                    if len(ax[i]) > 0:\n",
    "                        plt.plot(ax[i], ay[i], color=color_list[i], label=f'Fold {i + 1}')\n",
    "                plt.legend()\n",
    "                plt.pause(0.01)\n",
    "                # plt.ioff()\n",
    "            if train_loss >= 0.35:\n",
    "                p = 0\n",
    "\n",
    "                # min_valid_loss = valid_loss\n",
    "                # best_valid_acc = valid_acc\n",
    "\n",
    "            else:\n",
    "                p += 1\n",
    "\n",
    "                if p > n_patience:\n",
    "                    test_loss, test_acc = infer(test_loader, net, loss_func)\n",
    "                    print('test_loss = ', test_loss, 'test_acc = ', test_acc)\n",
    "                    # print(\"fold {} : train_acc= {}; valid_acc= {}; test_acc= {}\".format(f + 1, train_acc, valid_acc, test_acc))\n",
    "                    acc_list.append(test_acc)\n",
    "                    break\n",
    "        # test_loss, test_acc = infer(test_loader, net, loss_func)\n",
    "        # print('test_loss = ', test_loss, 'test_acc = ', test_acc)\n",
    "        # # print(\"fold {} : train_acc= {}; valid_acc= {}; test_acc= {}\".format(f + 1, train_acc, valid_acc, test_acc))\n",
    "        # acc_list.append(test_acc)\n",
    "\n",
    "    ave_test_acc = round(sum(acc_list) / len(acc_list), 5)\n",
    "    print(acc_list)\n",
    "    print(\"ave_test_acc =\", ave_test_acc)\n",
    "    if ave_test_acc > max_acc:\n",
    "        max_acc = ave_test_acc\n",
    "        print(\"max:\", max_acc)\n",
    "        torch.save(net, 'brainnetcnn.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
